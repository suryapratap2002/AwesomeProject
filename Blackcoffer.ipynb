{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDtsARefBiy6bVYrqaT4lk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surya2000PratapSingh/AwesomeProject/blob/main/Blackcoffer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm3hcBHA8UNR",
        "outputId": "a24ecd01-904b-47c6-9ce8-efd83ed0649e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('conll2002')"
      ],
      "metadata": {
        "id": "W5Rg5DGp8c_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_yGoYJxTv_2c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "input_data = pd.read_excel('/content/Input.xlsx')\n",
        "urls = input_data['URL'].tolist()\n",
        "url_ids = input_data['URL_ID'].tolist()\n"
      ],
      "metadata": {
        "id": "TeXLcsth6q9F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls"
      ],
      "metadata": {
        "id": "KzHGQHWt6tB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_ids"
      ],
      "metadata": {
        "id": "_yXeg-467CNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    article_title = soup.find('title').text.strip()\n",
        "    article_text = ''\n",
        "    paragraphs = soup.find_all('p')\n",
        "    for paragraph in paragraphs:\n",
        "        article_text += paragraph.text.strip() + ' '\n",
        "\n",
        "    return article_title, article_text\n",
        "\n",
        "articles_data = []\n",
        "for url, url_id in zip(urls, url_ids):\n",
        "    title, text = extract_article_text(url)\n",
        "    articles_data.append({'URL_ID': url_id, 'Title': title, 'Text': text})\n"
      ],
      "metadata": {
        "id": "00a6umNh7GQU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "def calculate_text_metrics(text):\n",
        "    blob = TextBlob(text)\n",
        "\n",
        "    positive_score = len([w for w in blob.words if TextBlob(w).sentiment.polarity > 0])\n",
        "    negative_score = len([w for w in blob.words if TextBlob(w).sentiment.polarity < 0])\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n",
        "    complex_word_count = len([w for w in blob.words if syllable_count(w) > 2])\n",
        "    percentage_complex_words = (complex_word_count / len(blob.words)) * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = len(blob.words) / len(sentences)\n",
        "    word_count = len(blob.words)\n",
        "    syllables_per_word = sum(syllable_count(w) for w in blob.words) / word_count\n",
        "    personal_pronouns = len([w for w in blob.words if w.lower() in ['i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'we', 'us', 'our', 'ours', 'ourselves', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves']])\n",
        "    avg_word_length = sum(len(w) for w in blob.words) / word_count\n",
        "\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllables_per_word, personal_pronouns, avg_word_length\n"
      ],
      "metadata": {
        "id": "UDXWaSuk7XVf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable_count(word):\n",
        "    word = word.lower()\n",
        "    if word in ['i', 'a', 'as', 'at', 'e', 'o', 'u', 'y']:\n",
        "        return 1\n",
        "    else:\n",
        "        return len(re.findall(r'[aeiouy]+', word))\n",
        "\n",
        "metrics_data = []\n",
        "for url, article in zip(urls, articles_data):\n",
        "    positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllables_per_word, personal_pronouns, avg_word_length = calculate_text_metrics(article['Text'])\n",
        "    metrics_data.append({\n",
        "        'URL_ID': article['URL_ID'],\n",
        "        'URL': url,\n",
        "        'Positive_Score': positive_score,\n",
        "        'Negative_Score': negative_score,\n",
        "        'Polarity_Score': polarity_score,\n",
        "        'Subjectivity_Score': subjectivity_score,\n",
        "        'Avg_Sentence_Length': avg_sentence_length,\n",
        "        'Percentage_of_Complex_Words': percentage_complex_words,\n",
        "        'FOG_Index': fog_index,\n",
        "        'Avg_Num_Words_Per_Sentence': avg_words_per_sentence,\n",
        "        'Complex_Word_Count': complex_word_count,\n",
        "        'Word_Count': word_count,\n",
        "        'Syllables_Per_Word': syllables_per_word,\n",
        "        'Personal_Pronouns': personal_pronouns,\n",
        "        'Avg_Word_Length': avg_word_length\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "metrics_df.to_excel('output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "9t15ZrnL7wUk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DmgU-dk0OKgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}